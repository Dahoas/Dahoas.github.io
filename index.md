---
title: Website
layout: template
filename: index.md
---

<img src="artifacts/alexh.jpg" class="portrait">

## About

<div>
Hi I'm Alex Havrilla. I am a PhD student at Georgia Tech studying both machine learning theory and application Machine Learning. I graduated from Carnegie Mellon University with a joint MS/BS in mathematics and an additional major in computer science.
</div>

<br/>

## Research Interests

My machine learning research encompasses a somewhat broad collection of both theoretical and experimental work undertaken with the ultimate goal of solving hard problems while simultaneously advancing human understanding. My experimental work most often attempts to explore novel connections between powerful techniques in machine learning via fast, principled experimentation. Examples of this type of work include studying various RL techniques for improving LLM reasoning and applications of Fourier Neural Operators to diffusion models for multi-resoltuion image synthesis. Right now, I am particularly interested in RL inspired techniques for improving LLM exploration and reasoning ability.

My theoretical work attempts to formalize the impressive learning abilities of large neural networks in mathematical/statistical frameworks which help deepen human understanding and motivate future experimental direction. This type of work most often establishes sample complexity bounds on the generalization error of neural networks solving various tasks.

# Papers

T. Sawada, D. Paleka, A. Havrilla, P. Vidas, A. Kranias, P. Tadepilli, A. Komatsuzaki, ARB: An Advanced Reasoning Benchmark for Large Language Modeling. To appear in Neurips 2023 MathAI workshop. <a href="https://arxiv.org/abs/2307.13692">[pdf]</a>

H. Liu, A. Havrilla, R. Lai, W. Liao. Deep Nonparmaetric Estimation of Intrinsic Data Structures by Chart Autoencoders: Generalization Error and Robustness. To appear in Journal of Applied and Computation Harmonic Analysis. <a href="https://arxiv.org/abs/2303.09863">[pdf]</a> 

A. Havrilla, M. Zhuravynski, A. Tiwari, J. Tow, E. Kim, Q. Anthony, S. Biderman, L. Castricato. trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback. To appear in EMNLP 2023.  

A. Havrilla, K. Rojas, W. Liao, M. Tao, Dual-FNO UNet: Scale-Robust Diffusion Model for Zero-Shot Super-Resolution Image Generation. To appear in Neurips 2023 workshop on diffusion models. <a href="https://github.com/Dahoas/Dahoas.github.io/blob/main/artifacts/FNO_Diffusion__arXiv_.pdf">[pdf]</a>

A. Havrilla, M. Iyer, Training Large Language Models with Noisy Algorithmic Chain of Thought. To appear in ICML 2023 worksohp on Symbolic and Data driven methods for reasoning in NLP. 

S. Biderman, A. Tikiwari, L. Castricato, E. Hallahan, A. Havrilla, Q. Anthony, E. Raff, What Makes a Good Multimodal Embedding Space? Theoretical and Empirical Insights from Topology. To appear on ArXiv.

A. Havrilla, L. Castricato, M. Shabuland, I. Yang, S. Frazier, M. Riedl, Robust Preference Learning for Storytelling via Contrastive Reinforcement Learning. To appear on ArXiv. <a href="https://arxiv.org/abs/2210.07792">[pdf]</a>

B. Dahal, A. Havrilla, M. Chen, T. Zhao, W. Liao, On Deep Generative models for Approximation and Estimation of Distributions on Manifolds. To appear in Neurips 2022. <a href="https://arxiv.org/abs/2302.13183">[pdf]</a>

A. Havrilla, P. Nayar, T. Tkocz, Khinchin-type inequalities via Hadamard's factorisation. To appear in International Mathematics Research Notices. <a href="https://arxiv.org/abs/2102.09500">[pdf]</a>

A. Havrilla, T. Tkocz, Sharp Khinchin-type inequalities for symmetric discrete uniform random variables. To appear in Israel J. Math. <a href="https://arxiv.org/abs/1912.13345">[pdf]</a>

My Thesis on Sharp Khintchine type Inequalities and some New Results <a href="https://github.com/Dahoas/Dahoas.github.io/blob/main/thesis_draft_3.pdf">[pdf]</a>